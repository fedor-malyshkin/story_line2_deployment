async: true
# Количество потоков на сайт (лучше не более 4)
crawler_per_site: 4
# Папка со скриптами для парсинга контента сайтов
script_dir: src/main/groovy/ru/nlp_project/story_line2/crawler/parser
# Блок с настройками для анализируемых сайтов
sites:
   # На каждый сайт. Название домена (будет использоваться для идентфикации и записи в БД)
 - domain: bnkomi.ru
   # На каждый сайт. стартовая страница для парсинга
   seed: http://www.bnkomi.ru/
# Каталог для сохранения данных о результатах парсинга сайта (для каждого сайта
# создается подпапка с именем домена для хранения соотвествующих данных)
storage_dir: /tmp/crawler
# строка подключения к MongoDB для сохранения результатаов анализа
connection_url: mongodb://localhost:27017/

# Logging settings.
logging:
  # The default level of all loggers. Can be OFF, ERROR, WARN, INFO, DEBUG, TRACE, or ALL.
  level: INFO
  # Logger-specific levels.
  loggers:
    "ru.nlp_project.story_line2.crawler": DEBUG

  appenders:
    - type: console
      threshold: TRACE
      target: stdout
    - type: file
      # The file to which current statements will be logged.
      currentLogFilename: crawler.log
      # When the log file rotates, the archived log will be renamed to this and gzipped. The
      # %d is replaced with the previous day (yyyy-MM-dd). Custom rolling windows can be created
      # by passing a SimpleDateFormat-compatible format as an argument: "%d{yyyy-MM-dd-hh}".
      archivedLogFilenamePattern: crawler-%d.log.gz
      # The number of archived files to keep.
      archivedFileCount: 5
